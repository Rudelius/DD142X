{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DD142X\n",
    "\n",
    "### Prerequisites\n",
    "* Python v3.7\n",
    "* Tensorflow v1.4.2 \n",
    "* Installera massa skit med pip för att få det att fungera allteftersom Python klagar.\n",
    "* Jag ändrade i train_image_classifier.py för att träna på CPU.\n",
    "* Typ 100 GB hårddiskutrymme.\n",
    "\n",
    "Det kan funka med andra versioner, men inte Tensorflow 2.\n",
    "\n",
    "### Ladda in directories\n",
    "Kör varje gång."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PRETRAINED_CHECKPOINT_DIR=/Users/johan/Dev/kexet/pretrained\n",
      "env: TRAIN_DIR=/Users/johan/Dev/kexet/checkpoints\n",
      "env: EVAL_DIR=/Users/johan/Dev/kexet/eval\n",
      "env: DATASET_DIR=/Users/johan/Dev/kexet/data\n"
     ]
    }
   ],
   "source": [
    "# Set pretrained checkpoint dir.\n",
    "%env PRETRAINED_CHECKPOINT_DIR=/Users/johan/Dev/kexet/pretrained\n",
    "\n",
    "# Where the training (fine-tuned) checkpoint and logs will be saved to.\n",
    "%env TRAIN_DIR=/Users/johan/Dev/kexet/checkpoints\n",
    "\n",
    "# Where the testing results will be saved to.\n",
    "%env EVAL_DIR=/Users/johan/Dev/kexet/eval\n",
    "\n",
    "# Where the dataset is saved to.\n",
    "%env DATASET_DIR=/Users/johan/Dev/kexet/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ladda ner en förtränad inception v3\n",
    "\n",
    "Kör bara första gången."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Download the model if you haven't yet.\n",
    "!mkdir ${PRETRAINED_CHECKPOINT_DIR}\n",
    "!wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz\n",
    "!tar -xvf inception_v3_2016_08_28.tar.gz\n",
    "!mv inception_v3.ckpt ${PRETRAINED_CHECKPOINT_DIR}\n",
    "!rm inception_v3_2016_08_28.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organisera ISIC datan\n",
    "\n",
    "Kör bara första gången."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Path to the raw ISIC pull.\n",
    "path_to_data = '/Users/johan/Dev/kexet/rawdata'\n",
    "\n",
    "# Final data path.\n",
    "final_path = '/Users/johan/Dev/kexet/data/'\n",
    "\n",
    "\n",
    "for root, dirs, files in os.walk(path_to_data):\n",
    "    # Do some basic filtering.\n",
    "    files = [f for f in files if not f[0] == '.']\n",
    "    files = [f for f in files if not f[-1] == 'g']\n",
    "    dirs[:] = [d for d in dirs if not d[0] == '.']\n",
    "    \n",
    "    for image_name in files:\n",
    "        image_path = root + '/' + image_name\n",
    "        try:\n",
    "            with open(image_path) as data_file:\n",
    "                # Read the disease from description file.\n",
    "                jsonList = json.load(data_file)\n",
    "                disease = jsonList['meta']['clinical']['diagnosis'].replace(' ', '_')\n",
    "                data_file.close()\n",
    "\n",
    "                # Make a directory for storing the data.\n",
    "                disease_dir = final_path + '/' + disease\n",
    "                if not os.path.exists(disease_dir):\n",
    "                    os.makedirs(disease_dir)\n",
    "\n",
    "                # Copy the image to the new dir.\n",
    "                image_src_path = path_to_data + '/Images/' + image_name + '.jpeg'\n",
    "                shutil.copy(image_src_path, disease_dir)\n",
    "\n",
    "        # Catch exceptions such as wrongly formated images and just skip it.\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dela in ISIC datan i training/test/validation\n",
    "Kör bara första gången."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "data_dir = \"/Users/johan/Dev/kexet/data\"\n",
    "\n",
    "# Over/under sample until we have this many training images of each class.\n",
    "n_training = 7500\n",
    "\n",
    "# Target fraction of original images for validation.\n",
    "frac_validation = 0.1\n",
    "\n",
    "# Target fraction of original images for testing.\n",
    "frac_test = 0.1\n",
    "\n",
    "# List of diseases we are studying with underscore instead of spaces.\n",
    "diseases = [\"melanoma\", \"nevus\", \"seborrheic_keratosis\"]\n",
    "\n",
    "\n",
    "\n",
    "# Loop through dirs to remove unwanted classes and create directory structure.\n",
    "for root, dirs, _ in os.walk(data_dir):\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        # Remove images of diseases not in disease list.\n",
    "        if dir_name not in diseases:\n",
    "            shutil.rmtree(os.path.join(root, dir_name))\n",
    "            \n",
    "        # Add test and validation directories.\n",
    "        else:\n",
    "            os.mkdir(os.path.join(root, \"test_\" + dir_name))\n",
    "            os.mkdir(os.path.join(root, \"validation_\" + dir_name))\n",
    "            \n",
    "        # Prefixa dirs med train_\n",
    "        os.rename(os.path.join(root, dir_name), os.path.join(root, \"train_\" + dir_name))         \n",
    "        \n",
    "        \n",
    "# Loop through dirs to move data into test/validation dirs.\n",
    "for root, dirs, _ in os.walk(data_dir):\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        # Loop through dirs prefixed train_\n",
    "        if dir_name.split('_', 1)[0] == \"train\":\n",
    "            for inner_root, _, files in os.walk(os.path.join(root, dir_name)):\n",
    "                \n",
    "                # Shuffle and then move images to validation/train\n",
    "                random.shuffle(files)\n",
    "                \n",
    "                # Validation/test size\n",
    "                n_validation = int(round(len(files)*frac_validation))\n",
    "                n_test = int(round(len(files)*frac_test))\n",
    "                \n",
    "                for i in range(n_validation):\n",
    "                    file = os.path.join(inner_root, files[i])\n",
    "                    move_to = os.path.join(root, \"validation_\" + dir_name.split('_', 1)[1])\n",
    "                    print(file + \" -> \" + move_to)\n",
    "                    shutil.move(file, move_to)\n",
    "                    \n",
    "                for i in range(n_test):\n",
    "                    file = os.path.join(inner_root, files[n_validation + i])\n",
    "                    move_to = os.path.join(root, \"test_\" + dir_name.split('_', 1)[1])\n",
    "                    shutil.move(file, move_to)\n",
    "\n",
    "\n",
    "# Loop through dirs to over/under sample the training data.\n",
    "for root, dirs, _ in os.walk(data_dir):\n",
    "    for dir_name in dirs:\n",
    "        \n",
    "        # Loop through dirs prefixed train_\n",
    "        if dir_name.split('_', 1)[0] == \"train\":\n",
    "            for inner_root, _, files in os.walk(os.path.join(root, dir_name)):\n",
    "           \n",
    "                # Duplicate images in dir if less than n_training.\n",
    "                n = 0\n",
    "                while len(files) + n < n_training:\n",
    "                    for image in files:\n",
    "                        n = n + 1\n",
    "                        original = os.path.join(inner_root, image)\n",
    "                        duplicate = os.path.join(inner_root, \"DUPE_\" + str(n) + \".jpeg\")\n",
    "                        print(original + \" -> \" + duplicate)\n",
    "                        shutil.copy(original, duplicate)\n",
    "                        if len(files) + n == n_training: break\n",
    "\n",
    "                print(\"Added \" + str(n) + \" to \" + dir_name + \". Previously \" + str(len(files)) + \" pictures.\")\n",
    "\n",
    "            # Remove images in dir if more than n_training.\n",
    "            for inner_root, _, files in os.walk(os.path.join(root, dir_name)):\n",
    "                n = 0\n",
    "                for image in files:\n",
    "                    if len(files) - n > n_training:\n",
    "                        n = n + 1\n",
    "                        print(\"Removing\" + os.path.join(inner_root, image))\n",
    "                        os.remove(os.path.join(inner_root, image))\n",
    "\n",
    "                print(\"Removed \" + str(n) + \" from \" + dir_name + \". Previously \" + str(len(files)) + \" pictures.\")\n",
    "\n",
    "\n",
    "# Finally loop through to print director summary.\n",
    "for root, dirs, _ in os.walk(data_dir):\n",
    "    for dir_name in dirs:\n",
    "        for inner_root, _, files in os.walk(os.path.join(root, dir_name)):\n",
    "            print(dir_name + \" n=\" + str(len(files)))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konvertera datan till Slim format.\n",
    "Kör bara första gången."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (download_and_convert_cancer.py, line 92)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.7/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3331\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-7ef3cd2588f0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import download_and_convert_cancer as dcc\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/johan/Dev/kexet/models/research/slim/download_and_convert_cancer.py\"\u001b[0;36m, line \u001b[0;32m92\u001b[0m\n\u001b[0;31m    class_name = os.path.basename(os.path.dirname(filenames[i])).split('_', 1)[1])\u001b[0m\n\u001b[0m                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import download_and_convert_cancer as dcc\n",
    "dcc.run(\"/Users/johan/Dev/kexet/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "* Ladda vikterna från den generella färdigtränade modellen, men exkludera sista lagret med \"checkpoint_exclude_scopes\". \n",
    "* Träna om vikterna i detta lager med \"trainable_scopes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune only the new layers for 1000 steps.\n",
    "!python3 train_image_classifier.py \\\n",
    "  --train_dir=${TRAIN_DIR} \\\n",
    "  --dataset_name=cancer \\\n",
    "  --dataset_split_name=train \\\n",
    "  --dataset_dir=${DATASET_DIR} \\\n",
    "  --model_name=inception_v3 \\\n",
    "  --checkpoint_path=${PRETRAINED_CHECKPOINT_DIR}/inception_v3.ckpt \\\n",
    "  --checkpoint_exclude_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\n",
    "  --trainable_scopes=InceptionV3/Logits,InceptionV3/AuxLogits \\\n",
    "  --max_number_of_steps=1000 \\\n",
    "  --batch_size=100 \\\n",
    "  --learning_rate=0.001 \\\n",
    "  --learning_rate_decay_type=fixed \\\n",
    "  --save_interval_secs=600 \\\n",
    "  --save_summaries_secs=600 \\\n",
    "  --log_every_n_steps=10 \\\n",
    "  --optimizer=rmsprop \\\n",
    "  --weight_decay=0.00004 \\\n",
    "  --opt_epsilon=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Run evaluation.\n",
    "!python3 eval_image_classifier.py \\\n",
    "  --checkpoint_path=/Users/johan/Dev/kexet/checkpoints/ \\\n",
    "  --eval_dir=${EVAL_DIR} \\\n",
    "  --dataset_name=cancer \\\n",
    "  --dataset_split_name=test \\\n",
    "  --dataset_dir=${DATASET_DIR} \\\n",
    "  --model_name=inception_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "\n",
    "path_to_walk = \"/Users/johan/Dev/kexet/checkpoints\"\n",
    "\n",
    "checkpoints = []\n",
    "\n",
    "for root, dirs, files in os.walk(path_to_walk):\n",
    "    for file in files:\n",
    "        valid = re.search('(.*ckpt-\\d\\d+)\\.[^\\.]+', file)\n",
    "\n",
    "        if valid:\n",
    "            checkpoints.append(path_to_walk + '/' + valid.group(1))\n",
    "\n",
    "checkpoints = list(dict.fromkeys(checkpoints))\n",
    "checkpoints.sort()\n",
    "print(checkpoints)\n",
    "\n",
    "\n",
    "for checkpoint in checkpoints:   \n",
    "    checkpoint_string = '--checkpoint_path=' + checkpoint\n",
    "    print(\"Evaluating \" + checkpoint_string)\n",
    "    \n",
    "    process = subprocess.Popen(['python3', 'eval_image_classifier.py',\n",
    "                                '--dataset_dir=/Users/johan/Dev/kexet/data',\n",
    "                                '--dataset_split_name=test',\n",
    "                                '--dataset_name=cancer',\n",
    "                                '--eval_dir=/Users/johan/Dev/kexet/eval', \n",
    "                                '--model_name=inception_v3', \n",
    "                                checkpoint_string], \n",
    "        stdout=subprocess.PIPE, \n",
    "        stderr=subprocess.PIPE)\n",
    "    stdout, stderr = process.communicate()\n",
    "    stdout, stderr\n",
    "    print(stdout)\n",
    "    print(stderr)\n",
    "    \n",
    "\n",
    "#                                \\\n",
    " #     --eval_dir=${EVAL_DIR} \\\n",
    "  #    --dataset_name=cancer \\\n",
    "   #   --dataset_split_name=test \\\n",
    "    #  --dataset_dir=${DATASET_DIR} \\\n",
    "     # --model_name=inception_v3 \\\n",
    "      #--checkpoint_path=' + checkpoint],*/\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import inception_preprocessing\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "\n",
    "def load_batch(dataset, batch_size=32, height=299, width=299, is_training=False):\n",
    "    \"\"\"Loads a single batch of data.\n",
    "    \n",
    "    Args:\n",
    "      dataset: The dataset to load.\n",
    "      batch_size: The number of images in the batch.\n",
    "      height: The size of each image after preprocessing.\n",
    "      width: The size of each image after preprocessing.\n",
    "      is_training: Whether or not we're currently training or evaluating.\n",
    "    \n",
    "    Returns:\n",
    "      images: A Tensor of size [batch_size, height, width, 3], image samples that have been preprocessed.\n",
    "      images_raw: A Tensor of size [batch_size, height, width, 3], image samples that can be used for visualization.\n",
    "      labels: A Tensor of size [batch_size], whose values range between 0 and dataset.num_classes.\n",
    "    \"\"\"\n",
    "    data_provider = slim.dataset_data_provider.DatasetDataProvider(\n",
    "        dataset, common_queue_capacity=32,\n",
    "        common_queue_min=8)\n",
    "    image_raw, label = data_provider.get(['image', 'label'])\n",
    "    \n",
    "    # Preprocess image for usage by Inception.\n",
    "    image = inception_preprocessing.preprocess_image(image_raw, height, width, is_training=is_training)\n",
    "    \n",
    "    # Preprocess the image for display purposes.\n",
    "    image_raw = tf.expand_dims(image_raw, 0)\n",
    "    image_raw = tf.image.resize_images(image_raw, [height, width])\n",
    "    image_raw = tf.squeeze(image_raw)\n",
    "\n",
    "    # Batch it up.\n",
    "    images, images_raw, labels = tf.train.batch(\n",
    "          [image, image_raw, label],\n",
    "          batch_size=batch_size,\n",
    "          num_threads=1,\n",
    "          capacity=2 * batch_size)\n",
    "    \n",
    "    return images, images_raw, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets import cancer\n",
    "from nets import inception\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "\n",
    "image_size = inception.inception_v3.default_image_size\n",
    "batch_size = 4\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    \n",
    "    dataset = cancer.get_split('test', '/Users/johan/Dev/kexet/data')\n",
    "    images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v3_arg_scope()):\n",
    "        logits, _ = inception.inception_v3(images, num_classes=dataset.num_classes, is_training=True)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(\"/Users/johan/Dev/kexet/checkpoints\")\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "            np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "    \n",
    "            for i in range(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                true_label = np_labels[i]\n",
    "                predicted_label = np.argmax(np_probabilities[i, :])\n",
    "                predicted_name = dataset.labels_to_names[predicted_label]\n",
    "                true_name = dataset.labels_to_names[true_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "                plt.axis('off')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Target number of training images per category after removing/duping.\n",
    "n_training = 7500\n",
    "\n",
    "# Target fraction of original images for validation.\n",
    "frac_validation = 0.1\n",
    "\n",
    "# Target fraction of original images for testing.\n",
    "frac_test = 0.1\n",
    "\n",
    "# List of diseases.\n",
    "diseases = [\"melanoma\", \"nevus\", \"seborrheic keratosis\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Loop through dirs.\n",
    "for root, dirs, _ in os.walk(\"/Users/johan/Dev/kexet/data\"):\n",
    "    for name in dirs:\n",
    "        if name not in diseases:\n",
    "            print(os.path.join(root, name))\n",
    "            #shutil.rmtree(os.path.join(root, name))\n",
    "            \n",
    "        \n",
    "        \"\"\"\n",
    "        # Check for dir prefixed train_\n",
    "        if name.split('_')[0] == \"train\":\n",
    "            for root2, _, files2 in os.walk(os.path.join(root, name)):\n",
    "                random.shuffle(files2)\n",
    "                \n",
    "                for i in range(int(len(files2)*frac_validation)):\n",
    "                    \n",
    "                    print (i)\n",
    "\n",
    "                \"\"\"\n",
    "                \n",
    "            \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        # Duplicate images in dir if less than n_training.\n",
    "        for root2, _, files2 in os.walk(os.path.join(root, name)):\n",
    "            n = 0\n",
    "            while len(files2) + n < n_training:\n",
    "                for name2 in files2:\n",
    "                    n = n + 1\n",
    "                    original = os.path.join(root2, name2)\n",
    "                    duplicate = os.path.join(root2, \"ISIC_\" + str(n) + \".jpeg\")\n",
    "                    print(\"Duplicating \" + original + \" - \" + duplicate)\n",
    "                    shutil.copy(original, duplicate)\n",
    "                    \n",
    "                    if len(files2) + n == n_training:\n",
    "                        break\n",
    "            print(\"Added \" + str(n) + \" to \" + name + \". Previously \" + str(len(files2)) + \" pictures.\")\n",
    "              \n",
    "        # Remove images in dir if more than n_training.\n",
    "        for root2, _, files2 in os.walk(os.path.join(root, name)):\n",
    "            n = 0\n",
    "            for name2 in files2:\n",
    "                if len(files2) - n > n_training:\n",
    "                    n = n + 1\n",
    "                    print(\"Removing \" + os.path.join(root2, name2))\n",
    "                    os.remove(os.path.join(root2, name2))\n",
    "                    \n",
    "            print(\"Removed \" + str(n) + \" from \" + name + \". Previously \" + str(len(files2)) + \" pictures.\")\n",
    "\n",
    "        \n",
    "        \n",
    "        print(\"Next \\n\")\n",
    "        \"\"\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import os\n",
    "import shutil\n",
    "\n",
    "for root, dirs, files in os.walk(\"/Users/johan/Dev/kexet/data\"):\n",
    "    for name in files:\n",
    "        print(name + \" \" + str(len(name)))\n",
    "        if len(name) != 17:\n",
    "            original = os.path.join(root, name)\n",
    "            #new = original + \".jpeg\"\n",
    "            os.remove(original)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
